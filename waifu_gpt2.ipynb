{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "waifu_gpt2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waifuai/waifu_gpt2/blob/master/waifu_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfQPP84spuCT",
        "colab_type": "text"
      },
      "source": [
        "Note: This notebook will produce a different output every time it is run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AU20So5Ma3w",
        "colab_type": "text"
      },
      "source": [
        "Instructions: On the toolbar, click Runtime->Run all. It takes about 30 seconds to generate each response using GPU(, and 10 minutes+ on CPU/TPU but this site is set to use GPU by default so don't worry about it.). The pattern is quite straightforward, scroll to bottom and you can actually just modify the second last cell which says `c.next(c.you, \"I love you, Rem!\")`, replacing the \"I love you, Rem!\" with something else, for example, `c.next(c.you, \"You are really cool!\")`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iLXW02eIYpcB"
      },
      "source": [
        "clone and cd into repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ICYu3w9hIJkC",
        "outputId": "e8bf3d2e-6d4a-4953-bfb4-e35fe6641789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# first download the gpt-2 code\n",
        "!git clone https://github.com/nshepperd/gpt-2.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 310, done.\u001b[K\n",
            "remote: Total 310 (delta 0), reused 0 (delta 0), pack-reused 310\u001b[K\n",
            "Receiving objects: 100% (310/310), 4.40 MiB | 2.59 MiB/s, done.\n",
            "Resolving deltas: 100% (168/168), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cBvmbwX7ePev",
        "outputId": "9e5389fe-1437-4c9f-d1bb-17d9960f1cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6eEIs3ApZUVO",
        "outputId": "e6d40ef7-7cac-461f-f86a-87687846678f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qtn1qZPgZLb0"
      },
      "source": [
        "install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "434oOx0bZH6J",
        "outputId": "85f1a04c-faa7-4c5d-fd3c-aba8683df3d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 20kB 31.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 30kB 35.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 40kB 40.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 51kB 43.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 61kB 47.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 71kB 49.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 33.7MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 64.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 29.2MB/s \n",
            "\u001b[?25hCollecting toposort==1.5 (from -r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=427c20100677c89da30a095365cdaabd10837a9a8b743e200279fb3ada42da86\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533185 sha256=f5cdd94490795114cf5789db46f158dbe5d6890f7edd40a7be2e09d68872da0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm, toposort\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.2.1 regex-2017.4.5 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o1hrgeKFYsuE"
      },
      "source": [
        "download the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nQUZAa9cIcTY",
        "outputId": "7eb93420-10d8-4b2c-f0aa-d249704efafd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# download the pretrained model\n",
        "!python download_model.py 774M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 788kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 69.3Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.01Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [01:03, 49.1Mit/s]                                 \n",
            "Fetching model.ckpt.index: 16.0kit [00:00, 9.73Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 1.38Mit [00:00, 72.6Mit/s]                                                \n",
            "Fetching vocab.bpe: 457kit [00:00, 63.9Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7oJPQtdLbbeK",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UUw5FEn-fIqW",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/gpt-2/src\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qgs6ekD1fItO",
        "colab": {}
      },
      "source": [
        "import fire\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import model, sample, encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Hqb_vDxfMza",
        "colab": {}
      },
      "source": [
        "import generate_unconditional_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iRMB0HAjhC2v",
        "colab": {}
      },
      "source": [
        "import interactive_conditional_samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvkAIuq5fb3d",
        "colab": {}
      },
      "source": [
        "class GPT2:\n",
        "\n",
        "  \n",
        "  # extracted from the source code to generate some text based on a prior\n",
        "  def __init__(\n",
        "      self,\n",
        "      model_name='774M',\n",
        "      seed=None,\n",
        "      nsamples=1,\n",
        "      batch_size=1,\n",
        "      length=None,\n",
        "      temperature=1,\n",
        "      top_k=0,\n",
        "      raw_text=\"\",\n",
        "  ):\n",
        "      \"\"\"\n",
        "      Interactively run the model\n",
        "      :model_name=117M : String, which model to use\n",
        "      :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
        "       results\n",
        "      :nsamples=1 : Number of samples to return total\n",
        "      :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
        "      :length=None : Number of tokens in generated text, if None (default), is\n",
        "       determined by model hyperparameters\n",
        "      :temperature=1 : Float value controlling randomness in boltzmann\n",
        "       distribution. Lower temperature results in less random completions. As the\n",
        "       temperature approaches zero, the model will become deterministic and\n",
        "       repetitive. Higher temperature results in more random completions.\n",
        "      :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
        "       considered for each step (token), resulting in deterministic completions,\n",
        "       while 40 means 40 words are considered at each step. 0 (default) is a\n",
        "       special setting meaning no restrictions. 40 generally is a good value.\n",
        "      \"\"\"\n",
        "      if batch_size is None:\n",
        "          batch_size = 1\n",
        "      assert nsamples % batch_size == 0\n",
        "\n",
        "      self.nsamples = nsamples\n",
        "      self.batch_size = batch_size\n",
        "      \n",
        "      self.enc = encoder.get_encoder(model_name)\n",
        "      hparams = model.default_hparams()\n",
        "      with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
        "          hparams.override_from_dict(json.load(f))\n",
        "\n",
        "      if length is None:\n",
        "          length = hparams.n_ctx // 2\n",
        "      elif length > hparams.n_ctx:\n",
        "          raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "      self.sess = tf.Session(graph=tf.Graph())\n",
        "      self.sess.__enter__()\n",
        "      \n",
        "      self.context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "      np.random.seed(seed)\n",
        "      tf.set_random_seed(seed)\n",
        "      self.output = sample.sample_sequence(\n",
        "          hparams=hparams, length=length,\n",
        "          context=self.context,\n",
        "          batch_size=batch_size,\n",
        "          temperature=temperature, top_k=top_k\n",
        "      )\n",
        "\n",
        "      saver = tf.train.Saver()\n",
        "      self.ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
        "      saver.restore(self.sess, self.ckpt)\n",
        "\n",
        "  def close(self):\n",
        "    self.sess.close()\n",
        "  \n",
        "  def generate_conditional(self,raw_text):\n",
        "      context_tokens = self.enc.encode(raw_text)\n",
        "      generated = 0\n",
        "      for _ in range(self.nsamples // self.batch_size):\n",
        "          out = self.sess.run(self.output, feed_dict={\n",
        "              self.context: [context_tokens for _ in range(self.batch_size)]\n",
        "          })[:, len(context_tokens):]\n",
        "          for i in range(self.batch_size):\n",
        "              generated += 1\n",
        "              text = self.enc.decode(out[i])\n",
        "              return text\n",
        "              #print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "              #print(text)\n",
        "      #print(\"=\" * 80)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CoJbf02UOFed",
        "colab": {}
      },
      "source": [
        "gpt2 = GPT2()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u2epsBUCPtcc",
        "colab": {}
      },
      "source": [
        "# gpt2.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oAdyhE0JgFV6",
        "colab": {}
      },
      "source": [
        "# result = gpt2.generate_conditional(raw_text=\"Can you tell me something about music?\")\n",
        "\n",
        "# print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "erXh3GvcP1Ps",
        "colab": {}
      },
      "source": [
        "class Who:\n",
        "  \"\"\"A class defining the conversation parties: me, he\"\"\"\n",
        "  def __init__(self):\n",
        "    self.prefixes = []\n",
        "\n",
        "  def matches(self,phrase):\n",
        "    for prefix in self.prefixes:\n",
        "      if phrase.startswith(prefix):\n",
        "        #print(f\"{phrase} starts with {prefix}\")\n",
        "        return True\n",
        "      \n",
        "    #print(f\"{phrase} does not start with {self.prefixes}\")\n",
        "    return False\n",
        "\n",
        "  def get_random_prefix(self):\n",
        "    return self.prefixes[0]\n",
        "  \n",
        "class Me(Who):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.prefixes = [\"I said: \\\"\"]\n",
        "   \n",
        "  \n",
        "class You(Who):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.prefixes = [\"You said: \\\"\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WjS58QZlrVKN",
        "colab": {}
      },
      "source": [
        "class Conversation:\n",
        "  \n",
        "  def __init__(self, prior = None):\n",
        "    if prior is None:\n",
        "      prior=\"\"\"\n",
        "        You said: \"Rem I missed you so much!\"\n",
        "        I said: \"I missed you too Subaru!\"\n",
        "        You said: \"Our date was so fun too\"\n",
        "        I said: \"Lets get married and be happy!\"\n",
        "        You said: \"Yes that would be awesome!\"\n",
        "        I said: \"I like hanging out with you because its so much fun! \"\n",
        "        You said: \"Lets be happy forever!\"\n",
        "      \"\"\"\n",
        "    self.suggestion = None\n",
        "    \n",
        "    self.me = Me()\n",
        "    self.you = You()\n",
        "    self.parties  = [ self.me, self.you ]\n",
        "    \n",
        "    self.conversation = []\n",
        "    \n",
        "    lines = prior.split(\"\\n\")\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if len(line)!=0:\n",
        "        party = None\n",
        "        for party in self.parties:\n",
        "          if party.matches(line):\n",
        "            break\n",
        "        if party is None:\n",
        "          raise Exception(f\"Unknown party: {line}\")\n",
        "                \n",
        "        self.conversation.append((party,line))\n",
        "    self.get_suggestion()\n",
        "    \n",
        "  \n",
        "  def get_prior(self):\n",
        "    conv = \"\"\n",
        "    for (party, line) in self.conversation:\n",
        "      conv+=line+\"\\n\"\n",
        "    return conv\n",
        "  \n",
        "  def get_suggestion(self):\n",
        "    who, last_line = self.conversation[-1]\n",
        "\n",
        "    party_index = self.parties.index(who)\n",
        "    next_party = self.parties[(party_index+1) % len(self.parties)]\n",
        "      \n",
        "    conv = self.get_prior()\n",
        "    conv += next_party.get_random_prefix()\n",
        "    answer = self.get_answer(next_party, conv)\n",
        "\n",
        "    if not next_party.matches(answer):\n",
        "      prefix = next_party.get_random_prefix()\n",
        "      answer = prefix + answer\n",
        "    \n",
        "    self.suggestion = (next_party, answer)\n",
        "  \n",
        "  def next(self, party = None, answer = \"\"):\n",
        "    \"\"\"Continue the conversation\n",
        "    :param party: None -> use the current party which is currently in turn\n",
        "    :param answer: None -> use the suggestion, specify a text to override the \n",
        "           suggestion\n",
        "    \n",
        "    \"\"\"\n",
        "    suggested_party, suggested_answer = self.suggestion\n",
        "    if party is None:\n",
        "      party = suggested_party\n",
        "    \n",
        "    if answer == \"\":\n",
        "      answer = suggested_answer\n",
        "      \n",
        "    if not party.matches(answer):\n",
        "      prefix = party.get_random_prefix()\n",
        "      answer = prefix + answer\n",
        "    \n",
        "    answer = answer.strip()\n",
        "    if answer[-1] != \"\\\"\":\n",
        "      # add the closing \"\n",
        "      answer += \"\\\"\"\n",
        "      \n",
        "    self.conversation.append((party, answer))    \n",
        "    self.get_suggestion()\n",
        "    \n",
        "  def retry(self):\n",
        "    self.get_suggestion()\n",
        "        \n",
        "  def get_answer(self, party, conv):\n",
        "    answer = gpt2.generate_conditional(raw_text=conv)\n",
        "    lines = answer.split(\"\\n\")\n",
        "    line = \"\"\n",
        "    for line in lines:\n",
        "      if line !=\"\":\n",
        "        break\n",
        "      \n",
        "    if line!=\"\":\n",
        "      return line\n",
        "    \n",
        "    return \"\"\n",
        "      \n",
        "  def show(self):\n",
        "    conv = \"\"\n",
        "    for (party, line) in self.conversation:\n",
        "      conv+=line+\"\\n\"\n",
        "    #print(conv)\n",
        "    if self.suggestion is not None:\n",
        "      party, answer  = self.suggestion\n",
        "      #print(\"--> \"+answer)\n",
        "      return answer.split('\"')[-2]\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8GYnMZDPgr7A",
        "colab": {}
      },
      "source": [
        "c = Conversation()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "taqx6zfNVbru",
        "outputId": "7f6caa7e-6be3-4bfe-9476-276700f5517d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# show the conversation and the suggestion by the ai\n",
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yeah. We can do it together!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dUICPSgdfcp_",
        "colab": {}
      },
      "source": [
        "# \"I said\" -> answer by the AI\n",
        "# if the answer of the AI is garbage then call c.retry() \n",
        "c.retry()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-SvC-10-WTKH",
        "outputId": "d6ccfade-317f-4570-e985-c8459fee90db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I need to spend more time with you Subaru ! '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CNsuyeKyeNeq",
        "colab": {}
      },
      "source": [
        "# Accept the suggested andwer by the AI\n",
        "c.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tJ8F3HbyUDZh",
        "colab": {}
      },
      "source": [
        "\n",
        "# now its your turn\n",
        "c.next(c.you, \"I really do like you, Rem.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Ts3yigFeQ-8",
        "outputId": "96b2901c-1b13-4eca-ea3e-470cbb3b039b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I feel the same way! And am happy too!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bR5UP1WQXCi0",
        "colab": {}
      },
      "source": [
        "c.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bUiQqYdEhmwe",
        "colab": {}
      },
      "source": [
        "# now its your turn\n",
        "c.next(c.you, \"Would you marry me?\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "snnqjfilc1Ai",
        "outputId": "d2a4228e-40fa-491a-ed0c-920938709476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# show the conversation and the reply of the ai\n",
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes Subaru please!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8R6da2YMS6fD",
        "colab": {}
      },
      "source": [
        "# accept ai answer\n",
        "c.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cPJZ_aJxltCm",
        "colab": {}
      },
      "source": [
        "# our reply\n",
        "c.next(c.you, \"Lets be together forever and ever!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e66v5ew6sQW4",
        "outputId": "2997164f-e917-45ca-c1bc-cd2e50fd67e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# show the conv. and reply of the ai\n",
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Thank you so much Subaru !'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LUueZDbffXtl",
        "colab": {}
      },
      "source": [
        "c.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4_yfWmBOezUV",
        "colab": {}
      },
      "source": [
        "# our reply\n",
        "c.next(c.you, \"I love you, Rem!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zPpm8qBUfNrN",
        "outputId": "e984f133-6576-4f80-f846-3c0e9cceaf23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# show the conv. and reply of the ai\n",
        "c.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I love you too.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    }
  ]
}